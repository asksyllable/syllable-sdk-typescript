/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod";
import { remap as remap$ } from "../../lib/primitives.js";
import { safeParse } from "../../lib/schemas.js";
import { Result as SafeParseResult } from "../../types/fp.js";
import { SDKValidationError } from "../errors/sdkvalidationerror.js";
import {
  PromptLlmProvider,
  PromptLlmProvider$inboundSchema,
  PromptLlmProvider$outboundSchema,
} from "./promptllmprovider.js";

/**
 * LLM configuration for a prompt.
 */
export type PromptLlmConfig = {
  /**
   * LLM API provider.
   */
  provider?: PromptLlmProvider | undefined;
  /**
   * Name of the model. Must match the deployment name in Azure AI Studio.
   */
  model?: string | undefined;
  /**
   * Optional model version.
   */
  version?: string | null | undefined;
  /**
   * Version of the provider's API.
   */
  apiVersion?: string | null | undefined;
  /**
   * Temperature parameter for the model. Determines randomness of responses - higher is more random, lower is more focused. Must be between 0.0 and 2.0, inclusive.
   */
  temperature?: number | null | undefined;
  /**
   * Controls the reproducibility of the job. The LLM will give the same or similar responses given the same inputs in multiple conversations with the same seed.
   */
  seed?: number | null | undefined;
};

/** @internal */
export const PromptLlmConfig$inboundSchema: z.ZodType<
  PromptLlmConfig,
  z.ZodTypeDef,
  unknown
> = z.object({
  provider: PromptLlmProvider$inboundSchema.optional(),
  model: z.string().default("gpt-4o"),
  version: z.nullable(z.string()).optional(),
  api_version: z.nullable(z.string()).optional(),
  temperature: z.nullable(z.number()).optional(),
  seed: z.nullable(z.number().int()).optional(),
}).transform((v) => {
  return remap$(v, {
    "api_version": "apiVersion",
  });
});

/** @internal */
export type PromptLlmConfig$Outbound = {
  provider?: string | undefined;
  model: string;
  version?: string | null | undefined;
  api_version?: string | null | undefined;
  temperature?: number | null | undefined;
  seed?: number | null | undefined;
};

/** @internal */
export const PromptLlmConfig$outboundSchema: z.ZodType<
  PromptLlmConfig$Outbound,
  z.ZodTypeDef,
  PromptLlmConfig
> = z.object({
  provider: PromptLlmProvider$outboundSchema.optional(),
  model: z.string().default("gpt-4o"),
  version: z.nullable(z.string()).optional(),
  apiVersion: z.nullable(z.string()).optional(),
  temperature: z.nullable(z.number()).optional(),
  seed: z.nullable(z.number().int()).optional(),
}).transform((v) => {
  return remap$(v, {
    apiVersion: "api_version",
  });
});

/**
 * @internal
 * @deprecated This namespace will be removed in future versions. Use schemas and types that are exported directly from this module.
 */
export namespace PromptLlmConfig$ {
  /** @deprecated use `PromptLlmConfig$inboundSchema` instead. */
  export const inboundSchema = PromptLlmConfig$inboundSchema;
  /** @deprecated use `PromptLlmConfig$outboundSchema` instead. */
  export const outboundSchema = PromptLlmConfig$outboundSchema;
  /** @deprecated use `PromptLlmConfig$Outbound` instead. */
  export type Outbound = PromptLlmConfig$Outbound;
}

export function promptLlmConfigToJSON(
  promptLlmConfig: PromptLlmConfig,
): string {
  return JSON.stringify(PromptLlmConfig$outboundSchema.parse(promptLlmConfig));
}

export function promptLlmConfigFromJSON(
  jsonString: string,
): SafeParseResult<PromptLlmConfig, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => PromptLlmConfig$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'PromptLlmConfig' from JSON`,
  );
}
